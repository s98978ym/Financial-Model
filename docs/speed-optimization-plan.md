# BM分析スピード最適化計画

## 現状分析

### 時間内訳（Phase 2 BM分析）

| 処理 | 所要時間 | 割合 |
|------|---------|------|
| LLM API呼び出し（推論） | 120〜300秒 | **~95%** |
| ドキュメント読込・截断 | 2〜3秒 | ~1% |
| Grounding検証 | ~1秒 | <1% |
| DB書き込み | 1〜2秒 | <1% |
| FE: 95%→100%遷移遅延 | 2〜4秒 | ~2% |

### ボトルネック根本原因

**LLM推論時間が全体の95%を占める。** 内訳:
- **入力トークン処理 (TTFT)**: ~6,500-7,000トークン → TTFT 5-15秒
- **出力トークン生成**: 3-5 proposals × 複雑なJSON → 推定5,000-10,000出力トークン → 100-250秒
- **95%停滞**: FEがprojectState再取得を待つ遅延 → 体感2-4秒追加

---

## 最適化施策（精度を下げない前提）

### 施策1: Anthropic Prompt Caching 導入 【高効果・低リスク】

**概要**: Anthropic APIの `cache_control` を使い、システムプロンプトをサーバー側キャッシュ

**対象ファイル**: `core/providers/anthropic_provider.py`

**変更内容**:
```python
# Before: system をプレーン文字列で渡している
"system": full_system,

# After: cache_control付きブロックで渡す
"system": [
    {
        "type": "text",
        "text": full_system,
        "cache_control": {"type": "ephemeral"},
    }
],
```

**効果**:
- システムプロンプト（~1,000トークン）のキャッシュで TTFT を3-5秒短縮
- 同一セッション内の再分析（フィードバック後）で特に効果大
- Anthropic側のキャッシュTTL: 5分
- コスト削減: キャッシュ読取は入力トークンの90%オフ

**精度影響**: なし（同じプロンプトが送られる）

---

### 施策2: max_tokens 最適化 【中効果・低リスク】

**概要**: `max_tokens` を 32,768 → 12,288 に削減

**対象ファイル**: `core/providers/base.py` (LLMConfig)

**根拠**:
- BusinessModelAnalysis の典型出力: 3-5 proposals で 4,000〜8,000 出力トークン
- 32Kは過剰なバッファ。12Kでも十分な余裕
- LLMが不必要に冗長な出力を生成するのを防止
- 異常に長い出力（evidence過多等）の生成時間を cap する

**効果**: 異常系での最大30-50秒短縮、通常系では5-15秒程度の改善

**精度影響**: なし（通常出力は12K以下に収まる。万が一切れた場合のフォールバックも既存の JSONOutputGuard で対応）

---

### 施策3: プロンプト最適化（入力トークン削減） 【中効果・低リスク】

**概要**: システムプロンプトとユーザープロンプトの重複排除・圧縮

**対象ファイル**: `src/agents/business_model_analyzer.py`

**具体的変更**:

1. **重複排除**: ハルシネーション防止ルールがシステム・ユーザー両方に記載 → ユーザー側の冗長な繰り返しを削除
2. **JSON例の圧縮**: ユーザープロンプトのJSON出力例にコメントが多数 → 最小限のスキーマ定義に置換
3. **パターンB/C例の除去**: `"...": "（同じ構造で別の解釈を提案）"` は不要

**削減見込み**: ユーザープロンプトのテンプレート部分 ~3,500文字 → ~2,200文字（約 -500トークン）

**効果**: TTFT を1-3秒短縮 + LLM処理全体で5-10秒短縮

**精度影響**: なし（冗長な説明の削除のみ、指示内容は保持）

---

### 施策4: フロントエンド95%停滞の解消 【中効果・低リスク】

**概要**: ジョブ完了時にresult_refを使って直接結果を取得し、projectState再取得を待たない

**対象ファイル**:
- `services/api/app/routers/jobs.py` — GET /jobs/{job_id} のレスポンスに result_ref を含める
- `apps/web/src/lib/usePhaseJob.ts` — ジョブ完了時の result 取得を高速化
- `apps/web/src/lib/api.ts` — phase_result 直接取得の API 追加

**変更内容**:
```typescript
// usePhaseJob.ts: ジョブ完了時に即座に結果を取得
if (jobData.status === 'completed' && jobData.result_ref) {
  const result = await api.getPhaseResult(jobData.result_ref)
  setState(prev => ({
    ...prev,
    status: 'completed',
    progress: 100,
    result: result.raw_json,
  }))
}
```

**効果**: 95%→100%の遷移を2-4秒 → <1秒に短縮

**精度影響**: なし（UIロジック変更のみ）

---

### 施策5: ハートビート間隔の短縮 【低効果・低リスク】

**概要**: ハートビート間隔を8秒→4秒に変更

**対象ファイル**: `services/worker/tasks/phase2.py`

**変更**: `_heartbeat_stop.wait(timeout=8)` → `_heartbeat_stop.wait(timeout=4)`

**効果**: UI上の進捗更新がよりスムーズに。体感速度の改善

**精度影響**: なし

---

## 効果サマリー

| 施策 | 短縮見込み | 実装難易度 | リスク |
|------|-----------|-----------|--------|
| 1. Prompt Caching | 3〜5秒 (TTFT) | 低 | なし |
| 2. max_tokens最適化 | 5〜50秒 | 極低 | 極低 |
| 3. プロンプト圧縮 | 5〜10秒 | 中 | なし |
| 4. FE 95%停滞解消 | 2〜4秒 (体感) | 中 | 低 |
| 5. ハートビート短縮 | 0秒 (体感向上) | 極低 | なし |

**合計見込み**: 現状120〜300秒 → **100〜240秒（15-20%短縮）** + 体感UX大幅改善

---

## 実装順序

```
Phase A（即効・低リスク）
  ├─ 施策2: max_tokens → 12,288
  └─ 施策5: ハートビート 8s → 4s

Phase B（中効果・要テスト）
  ├─ 施策1: Prompt Caching
  └─ 施策3: プロンプト圧縮

Phase C（UX改善）
  └─ 施策4: FE 95%停滞解消
```

---

## 追加検討事項（今回スコープ外）

| 案 | 理由でスコープ外 |
|----|-----------------|
| モデル変更 (Haiku等) | 精度低下のリスクが高い |
| 2段階LLM（Haiku→Sonnet） | 複雑化+合計時間が増える可能性 |
| ドキュメント截断上限の縮小 | 精度影響の検証が必要 |
| WebSocket化 | 大規模改修が必要 |
| Phase並列実行 | 各Phase間に依存関係あり |
